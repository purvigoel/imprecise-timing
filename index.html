<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, shrink-to-fit=no">
    <title>Generative Motion Infilling From Imprecisely Timed Keyframes</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.5.0/css/bootstrap.min.css">
    <link href='https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,500,600' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" href="./assets/css/styles.css">

    <link rel="apple-touch-icon" sizes="180x180" href="./assets/media/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="./assets/media/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="./assets/media/favicon-16x16.png">
    <link rel="manifest" href="./assets/media/site.webmanifest">

    <meta property="og:site_name" content="Generative Motion Infilling From Imprecisely Timed Keyframes" />
    <meta property="og:type" content="video.other" />
    <meta property="og:title" content="Generative Motion Infilling From Imprecisely Timed Keyframes" />
    <meta property="og:description" content="Generative Motion Infilling From Imprecisely Timed Keyframes, 2025." />
    <script type="module" src="https://unpkg.com/@google/model-viewer@2.0.1/dist/model-viewer.min.js"></script>
    <script src="https://polyfill.io/v3/polyfill.js?features=IntersectionObserver"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.5.0/js/bootstrap.bundle.min.js"></script>
    <script src="https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/js/webflow.fd002feec.js"></script>
    <script src="./assets/scripts/main.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/sticksy/dist/sticksy.min.js"></script>
</head>

<body class="noscroll">
    <div class="button-bar text-light" style="padding-bottom: 10px; background-color: rgb(35, 35, 35);">
        <div class="container" style="max-width: 768px;">
            <h1 class="text-center">Generative Motion Infilling<br> From Imprecisely Timed Keyframes</h1>
        </div>
        <div class="container" style="max-width: 768px;">
            <h3 class="text-center" stye="font-size:2rem;">Eurographics 2025</h3>
        </div>
        <div class="container" style="max-width: 768px; padding-bottom: 20px;">
            <div class="row authors">
                <div class="col">
                    <h5 class="text-center"><a href="https://www.purvigoel.com/">Purvi Goel<sup>1</sup></a></h5>
                    <h6 class="text-center"></h6>
                </div>
                <div class="col">
                    <h5 class="text-center"><a href="https://cs.stanford.edu/~haotianz/">Haotian Zhang<sup>2</sup></a></h5>
                    <h6 class="text-center"></h6>
		        </div>
                <div class="col">
                    <h5 class="text-center"><a href="https://tml.stanford.edu/">C. Karen Liu<sup>1</sup></a></h5>
                    <h6 class="text-center"></h6>
		</div>
		<div class="col">
                    <h5 class="text-center"><a href="https://graphics.stanford.edu/~kayvonf/">Kayvon Fatahalian<sup>1</sup></a></h5>
                    <h6 class="text-center"></h6>
                </div>
            </div>
        </div>

        <div class="container" style="max-width: 768px; padding-bottom: 0px;">
            <div class="row authors">
                <div class="col">
                    <h6 style="margin-top: 10px;" class="text-center"><sup>1</sup>Stanford University, <sup>2</sup>NVIDIA</h6>
                </div>
            </div>
        </div>
        <div class="buttons" style="margin-bottom: 8px;">
            <a class="btn text-light" role="button" href="http://arxiv.org/abs/2503.01016">
                Paper
            </a>
            <a class="btn text-light" role="button" href="http://arxiv.org/abs/2503.01016">
                Supplement
            </a>
            <a class="btn text-light" role="button" href="">
                Code (Coming Soon)
            </a>
            <a class="btn text-light" role="button" href="">
                Video (Coming Soon)
            </a>
            <a class="btn text-light" role="button" href="https://medium.com/@goel.purvi/the-missing-piece-animation-timing-in-ai-driven-motion-inbetweening-779f12eab458">
                Blog Post
            </a>
        </div>
    </div>
    
    <div style="background-color: rgb(255,255,255);">
        

        <div class="container" style="max-width: 768px; padding-top: 30px; padding-bottom: 0px;">
            <div class="row">
                <div class="col-md-12">
                    
                    <p>
                        <!-- <strong> -->
                Keyframes are a standard representation in character animation, and recent motion-inbetweening methods use them to control generative motion models. However, specifying keyframe <b>timing</b> is challenging in practice. 
                We introduce a system for motion-inbetweening in the context of keyframes that may be imprecisely timed. 
                Our key idea is a novel model architecture that explicitly outputs a time-warping function to <b>correct mistimed keyframes</b>, 
                and the spatial details to add in between those keyframes. Our system is able to generate high-quality motion from mistimed keyframes, 
                and supports both motion synthesis and editing workflows.
                        <!-- </strong> -->
                    </p>
                    

                </div>
            </div>
        </div>
        <div class="container" style="max-width: 768px; padding-top: 10px;  ">
            <div class="row">
                <div class="col-md-12">
                    <img src="assets/images/banner.png" style="width:100%;">
                </div>
            </div>
        </div>
        <div class="button-bar" style="padding-top: 0px; padding-bottom: 0px; background-color: rgb(255,255,255);">
            <center> <p>Jump to:</p></center>
            <div class="buttons" style="margin-bottom: 8px; margin-top: 0px;">
                <a class="btn btn-black-outline" role="button" href="#section0" style="color: black; border: 1px solid black;">
                    Motivation
                </a>
                <a class="btn btn-black-outline" role="button" href="#section1" style="color: black; border: 1px solid black;">
                    Problem Illustration
                </a>
                <a class="btn btn-black-outline" role="button" href="#section2" style="color: black; border: 1px solid black;">
                    Method Summary
                </a>
                <a class="btn btn-black-outline" role="button" href="#section3" style="color: black; border: 1px solid black;">
                    Results (Synthesis)
                </a>
                <a class="btn btn-black-outline" role="button" href="#section4" style="color: black; border: 1px solid black;">
                    Results (Editing)
                </a>
                <a class="btn btn-black-outline" role="button" href="#section5" style="color: black; border: 1px solid black;">
                    FAQ/Hindsights
                </a>
            </div>
        </div>
    </div>

    <div id="section0" style="background-color: rgb(240, 240, 240);">
        <div class="container" style="padding-top: 30px; padding-bottom: 30px; text-align:center;">
            <p><b>Motivation.</b> Machine-learning-based motion in-betweening solutions can generate natural motion from sparse keyframe constraints.
                The problem is that while casual users may be able to specify the poses when setting keyframe constraints for the model, timing those keyframes can be extremely challenging [Terra 2004]. 
                Even experienced animators have noted the importance and difficulty of animation timing.
            
                When input keyframes have imprecise timing, standard motion in-betweening solutions--trained to match input keyframes exactly--can generate unrealistic or undesireable motion. <br><br>
                We believe learned in-betweening systems must be capable of adjusting the timing of the input keyframes.</div>
    </div>
    
    <div id="section1" style="background-color: rgb(240, 240, 240);"></div>
        <div class="container" style="padding-top: 30px; padding-bottom: 30px; text-align:center;">
            <p><b>Problem Illustration:</b> To generate a motion like the dynamic dance on the left (top left), we give six keyframes (top right) to a standard motion-inbetweening model. 
                If the keyframe timing is incorrect, the model can generate jerky, unrealistic motion with a missed step (bottom right). Our model generates the desired motion even if keyframe timing is incorrect (bottom left).  </p>
        </div>
    </div>

    <div style="background-color: rgb(255, 255, 255); padding-top: 30px; " >
        <div class="video-row">
            <div class="col" style="padding-left:30%; padding-bottom:20px; display:grid;">
                The desired motion.
                <video class="specialvideo video lazy img-fluid" autoplay muted loop playsinline controls style="width:100%">
                    <source src="./assets/video/problem_illus/original.mp4" type="video/mp4"></source>
                </video>
            </div>
            <div class="col" style="padding-right:20%; padding-bottom:20px; display:grid;">
                <video class="specialvideo video lazy img-fluid"  muted loop playsinline controls style="width:100%">
                    <source src="./assets/video/problem_illus/keyframes.mp4" type="video/mp4"></source>
                </video>
                Six poses from the desired motion, to act as keyframe constraints for inbetweening model.
            </div>
        </div>
        <div class="video-row">
            <div class="col" style="padding-left:30%; display:grid;">
                <video class="specialvideo video lazy img-fluid" muted loop playsinline controls style="width:100%">
                    <source src="./assets/video/problem_illus/inbetween.mp4" type="video/mp4"></source>
                </video>
                Motion generated by a standard motion-inbetweening model, given imprecisely timed keyframes.
            </div>
            <div class="col" style="padding-right:20%; display:grid;">
                <video class="specialvideo video lazy img-fluid" muted loop playsinline controls style="width:100%">
                    <source src="./assets/video/problem_illus/ours.mp4" type="video/mp4"></source>
                </video>
                Motion generated by our model, given imprecisely timed keyframes.
            </div>
        </div>
        </div>
    </div>

    <div id="section2" style="background-color: rgb(240, 240, 240);">
        <div class="container" style="padding-top: 30px; padding-bottom: 30px; text-align:center;">
            <p> <b>Method Summary.</b> First, we use our proposed data generation procedure to create a dataset of deliberately mistimed keyframes and their corresponding ground truth motion (left). 
                
                Our goal is to train a model that can learn to generate the ground truth motion from the mistimed keyframes.
                So, we introduce a novel dual-headed transformer-based diffusion model (right) that jointly predicts an <b>explicit global time-warping function</b> to correct and retime the mistimed keyframes,
            and <b>local pose residuals</b> that add spatial details to the motion.
        </p>
        </div>
    </div>


    <div style="background-color: rgb(255, 255, 255);">
        <div class="container" style="padding-top: 30px; padding-bottom: 30px; display: flex; justify-content: center; gap: 20px;">
            <div style="border-right: 2px solid black; padding-right: 10px;">
                <img src="assets/images/dataset_gen.png" style="width: 100%;">
            </div>
            <img src="assets/images/model.png" style="width: 40%;">
           
        </div>
    </div>


    <div id="section3" style="background-color: rgb(240, 240, 240);">
        <div class="container" style="padding-top: 30px; padding-bottom: 30px; text-align:center;">
            <p><b>Results (Synthesis): </b> Our model can generate high-quality motions from imprecisely timed keyframes. We show the set of input keyframes on the left (red),
                 which may have imprecise timing. We show the generated motion from a state-of-the-art motion-inbetweening model (middle), which treats the timing of
                 input keyframes as hard constraints. We show the generated motion from our model (right). </p>
        </div>
    </div>
    <!-- Big grid -->
    <div style="background-color: rgb(255, 255, 255); padding-top: 30px; " >
        <div class="video-row">
            <div class="col" style="flex: 1; padding-left: 0%; padding-right: 10px; display: grid;">
                Set of keyframes specifying a motion of a person doing arm circles. In this case, the keyframes were sampled from a ground truth motion and deliberately mistimed.
                <video class="specialvideo video lazy img-fluid" autoplay muted loop playsinline controls style="width: 100%;">
                    <source src="./assets/video/results/arm_circles_cond.mp4" type="video/mp4"></source>
                </video>
            </div>
        
            <div class="col" style="flex: 1; padding-left: 10px; padding-right: 10px; display: grid;">
                Motion generated by a prior learned motion-inbetweening model, which treats the timing of input keyframes as a hard constraint. Due to the imprecise timing of input keyframes, the model fails to create circles.
                <video class="specialvideo video lazy img-fluid" muted loop playsinline controls style="width: 100%;">
                    <source src="./assets/video/results/arm_circles_condmdi.mp4" type="video/mp4"></source>
                </video>
            </div>
        
            <div class="col" style="flex: 1; padding-left: 10px; padding-right: 0%; display: grid;">
                Motion generated by our model. The model is able to generate high-quality motion and recovers the circles.
                <video class="specialvideo video lazy img-fluid" muted loop playsinline controls style="width: 100%;">
                    <source src="./assets/video/results/arm_circles_ours.mp4" type="video/mp4"></source>
                </video>
            </div>
        </div>
        <div class="video-row">
            <div class="col" style="flex: 1; padding-left: 0%; padding-right: 10px; display: grid;">
                Set of keyframes specifying a motion of a person writing on a whiteboard. In this case, there was no ground truth motion; instead, we placed poses 
                manually on the timeline, mimicking typical user interactions with the system.
                <video class="specialvideo video lazy img-fluid" autoplay muted loop playsinline controls style="width: 100%;">
                    <source src="./assets/video/results/whiteboard_keyfs.mp4" type="video/mp4"></source>
                </video>
            </div>
        
            <div class="col" style="flex: 1; padding-left: 10px; padding-right: 10px; display: grid;">
                Because the timing of the keyframes was ultimately innaccurate, a model which treats the timing of input keyframes as a hard constraint produces a motion that is jerky and unrealistic..
                <video class="specialvideo video lazy img-fluid" muted loop playsinline controls style="width: 100%;">
                    <source src="./assets/video/results/whiteboard_condmdi.mp4" type="video/mp4"></source>
                </video>
            </div>
        
            <div class="col" style="flex: 1; padding-left: 10px; padding-right: 0%; display: grid;">
                Meanwhile, our model creates smooth steps and a natural transition.
                <video class="specialvideo video lazy img-fluid" muted loop playsinline controls style="width: 100%;">
                    <source src="./assets/video/results/whiteboard_ours.mp4" type="video/mp4"></source>
                </video>
            </div>
        </div>
        </div>
    </div>

     <div id="section4" style="background-color: rgb(240, 240, 240);">
        <div class="container" style="padding-top: 30px; padding-bottom: 30px; text-align:center;">
            <p><b>Results: Editing</b>. Our model can also be used to edit existing motions. In editing scenarios, 
                handling imprecise timing is very important: the original motion already has an existing timing, and introducing an edit may require changing that timing.
        </div>
    </div>

    <div style="background-color: rgb(255, 255, 255); padding-top: 30px; " >
            <div class="video-row">
                <div class="col" style="padding-left:30%; display:grid;">
                    Given an original motion of a person walking in a crouched manner we want the character to walk farther but in the same crocuhed style.
                    <video class="specialvideo video lazy img-fluid" muted loop playsinline controls style="width:100%">
                        <source src="./assets/video/results/edit_original.mp4" type="video/mp4"></source>
                    </video>
                </div>
                <div class="col" style="padding-right:20%; display:grid;">
                    We sample 4 keyframes from the original motion and space them out further apart.
                    <video class="specialvideo video lazy img-fluid" muted loop playsinline controls style="width:100%">
                        <source src="./assets/video/results/edit_keyfs.mp4" type="video/mp4"></source>
                    </video>
                </div>
            </div>
            <div class="video-row">
                <div class="col" style="padding-left:30%; display:grid;">
                    A model which treats the timing of input keyframes as a hard constraint reacts to the imprecisely timed middle keyposes by generating motion with an uneven gait
                    <video class="specialvideo video lazy img-fluid" muted loop playsinline controls style="width:100%">
                        <source src="./assets/video/results/edit_condmdi.mp4" type="video/mp4"></source>
                    </video>
                </div>
                <div class="col" style="padding-right:20%; display:grid;">
                    Given the input keyposes, our model generates a new motion (bottom, right) that is similar to the original motion, but walks farther.
                    <video class="specialvideo video lazy img-fluid" muted loop playsinline controls style="width:100%">
                        <source src="./assets/video/results/edit_ours.mp4" type="video/mp4"></source>
                    </video>
                </div>
            </div>
        </div>
    </div>

    <div id="section5" style="background-color: rgb(240, 240, 240);">
        <div class="container" style="max-width: 768px; padding-top: 30px;">
            <div class="row">
                <div class="col-md-12">
                    <h2>FAQ:</h2>


                    <p> <b>How does keyframe density affect controllability, and what impact does it have on prior approaches' ability to handle mistimed keyframes?</b></p>
                     Existing motion inbetweening solutions are trained to match input keyposes at exactly the frame provided. 
                     In practice, this hard timing constraint does not pose much of a problem to priorlearned motion-inbetweening models if there are only two or three keyframe constraints, since the model has significant flexibility to construct a 
                     motion in between the keyframes that still looks natural.
                     But given that the model is very sparsely constrained, the generated motion--despite appearing natural and adhering to constraints--may  not
                      reflect the final detailed animation that the animator envisioned. If the animator seeks more control by providing more keyframes, 
                      the model has less flexibility to compensate for mistimed keyframe inputs. The result from standard motion-inbetweening solutions is output that may feature unrealistic dynamics 
                      (the character moves from one keyframe to another too fast) or fail to hit keyframes (the character does not have enough time to reach the next keyframe).
                      <br><br>
                    <p> <b>Why not learn the retiming function and spatial details with two separate models? </b></p>
                    One alternative model architecture is to learn a time-warping function separately from spatial details. For example, a model could learn a timewarping function that warps the input keyframes to match the ground truth motion. 
                    The retimed keyframes could then be used to generate motion using a standard motion-inbetweening model.
                    
                    However, in many cases, timing and spatial details are actually closely related, e.g., 
                    a higher jump may require more time in the air, but also more wind-up/knee bend. This is primarily why we propose a dual-headed model that jointly learns a time-warping function and spatial details.
                    <br><br>

                     <p> <b>Why not just use a previous learned motion-inbetweening model, and retime the generated output manually? </b></p>
                     Instead of relying on a model that learns to retime keyframes during the generation process, one might ask: could a vanilla motion-inbetweening model be applied to imprecisely timed keyframes, with the generated motion then manually retimedâ€”say, using a handcrafted timewarp function?

                     Unfortunately, using prior learned interpolation techniques on imprecisely timed keyframes can often results in motion that lacks accurate spatial detail, which is hard to recover through manual retiming. This limitation underscores the need for an integrated approach. 
                     <br><br>
                     <p> <b>Where does this fit in the context of other forms of conditioned generative motion generation, like text-to-motion?</b></p>
                     We believe that the ideal future motion editing system should be multimodal, incorporating both text and kinematic joint constraints (and image/video-based demonstration, and scene-specific details, and more). Each modality offers unique advantages: text is highly accessible but can be ambiguous, 
                     while kinematic joint constraints allow for greater control but can be challenging to articulate. 
                     We believe a multimodal system that combined both would get the best of both worlds: using text to express high-level intent and kinematic constraints
                      (e.g., keyframes) to communicate more precise details.
                       In this work, we focus on a fundamental challenge in specifying kinematic constraints: 
                       defining the timing of these constraints. Ideally, this would make it easier to control any animation system, multimodal or not, that exposes kinematic constraints.
                        <br><br>
                       TLDR: Both! Both is good!
                       <br><br>

                    <h4> <b>Hindsights?</b></h4>
                        Still brewing...
                    <br><br>

                </div> 
            </div>
        </div>
    </div>

    <div style="background-color: rgb(30, 30, 30);">
        <div class="container text-light" style="max-width: 768px; padding: 30px 0px;">
            <div class="row">
                <div class="col-md-12">
                    <h2>Acknowledgements:</h2>
                    Purvi Goel is supported by a Stanford Interdisciplinary Graduate Fellowship. We thank the anonymous reviewers for constructive feedback; Vishnu Sarukkai, Sarah Jobalia, Sofia Di Toro Wyetzner, and Mia Tang for proofreading; James Hong, Zander Majercik, and David Durst for helpful discussions; Meta for gift support.

                     This website was developed referencing <a href="https://edge-dance.github.io/">EDGE</a>, and by extension, the excellent <a href="https://imagen.research.google/">Imagen</a> site.
                </div>
            </div>
        </div>
    </div>
</body>

</html>
